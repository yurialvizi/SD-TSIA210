{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iafPdtuncbq7"
   },
   "source": [
    "# TP: MNIST with Neural Networks (NN)\n",
    "\n",
    "## Students\n",
    "- Raynner Schnneider Carvalho\n",
    "- Yuri de Sene Alvizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlKZ3Hnas7B4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"Using tensorflow version \" + str(tf.__version__))\n",
    "print(\"Using keras version \" + str(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_QLz9_jbRZq"
   },
   "source": [
    "## Loading and preparing the MNIST dataset\n",
    "Load the MNIST dataset made available by keras.datasets. Check the size of the training and testing sets. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "gG83hGyVmijn"
   },
   "outputs": [],
   "source": [
    "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
    "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
    "from keras.datasets import ...\n",
    "(train_images, train_labels), (test_images, test_labels) = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRPbU_Z4U6Ac"
   },
   "source": [
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "Using the pyplot package, visualize the first sample of the training set:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VAu7oW0Zu4"
   },
   "outputs": [],
   "source": [
    "# Let us visualize the first training sample using the Matplotlib library with the imshow function\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7YsRekMVDg-"
   },
   "source": [
    "The database contains images of handwritten digits. Hence, they belong to one of 10 categories, depending on the digit they represent. \n",
    "Reminder: in order to do multi-class classification, we use the softmax function, which outputs a multinomial probability distribution. That means that the output to our model will be a vector of size $10$, containing probabilities (meaning that the elements of the vector will be positive sum to $1$).\n",
    "For easy computation, we want to true labels to be represented with the same format: that is what we call **one-hot encoding**. For example, if an image $\\mathbf{x}$ represents the digit $5$, we have the corresponding one_hot label (careful, $0$ will be the first digit): \n",
    "$$ \\mathbf{y} = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] $$\n",
    "Here, you need to turn train and test labels to one-hot encoding using the following function: \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQbkllF8mnaf"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jv29YLtVO3q"
   },
   "source": [
    "Images are black and white, with size $28 \\times 28$. We will work with them using a simple linear classification model, meaning that we will have them as vectors of size $(784)$.\n",
    "You should then transform the images to the size $(784)$ using the numpy function ```reshape```.\n",
    "\n",
    "Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation. Be careful to your methodology: while you have access to training data, you may not have access to testing data, and must avoid using any statistic on the testing dataset.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptTRSDo5nJyZ"
   },
   "outputs": [],
   "source": [
    "# Reshape images to vectors of pixels\n",
    "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
    "train_images = train_images.reshape(...)\n",
    "\n",
    "\n",
    "\n",
    "# Cast pixels from uint8 to float32\n",
    "train_images = train_images.astype('float32')\n",
    "\n",
    "# Now let us normalize the images so that they have zero mean and standard deviation\n",
    "# Hint: are real testing data statistics known at training time ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part: working with Numpy\n",
    "\n",
    "Look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf) for some basic information on how to use numpy.\n",
    "\n",
    "### Defining the model \n",
    "\n",
    "We will here create a simple, linear classification model. We will take each pixel in the image as an input feature (making the size of the input to be $784$) and transform these features with a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$. Since there is $10$ possible classes, we want to obtain $10$ scores. Then, \n",
    "$$ \\mathbf{W} \\in \\mathbb{R}^{784 \\times 10} $$\n",
    "$$ \\mathbf{b} \\in \\mathbb{R}^{10} $$\n",
    "\n",
    "and our scores are obtained with:\n",
    "$$ \\mathbf{z} = \\mathbf{W}^{T} \\mathbf{x} +  \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^{784}$ is the input vector representing an image.\n",
    "We note $\\mathbf{y} \\in \\mathbb{R}^{10}$ as the target one_hot vector. \n",
    "\n",
    "Here, you fist need to initialize $\\mathbf{W}$ and $\\mathbf{b}$ using ```np.random.normal``` and ```np.zeros```, then compute $\\mathbf{z}$.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid implementing a complicated gradient back-propagation,\n",
    "# we will try a very simple architecture with one layer \n",
    "def initLayer(n_input,n_output):\n",
    "    \"\"\"\n",
    "    Initialize the weights, return the number of parameters\n",
    "    Inputs: n_input: the number of input units - int\n",
    "          : n_output: the number of output units - int\n",
    "    Outputs: W: a matrix of weights for the layer - numpy ndarray\n",
    "           : b: a vector bias for the layer - numpy ndarray\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create W at the right size with a normal distribution\n",
    "    W = ..\n",
    "    # Create b at the right size, with zeros\n",
    "    b = ..\n",
    "    nb_params = ..\n",
    "    return W, b, nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = train_images.shape[0] \n",
    "n_feature = train_images.shape[1] * train_images.shape[2]\n",
    "n_labels = 10\n",
    "W, b, nb_params = initLayer(n_feature, n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, b, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: W: the weights - numpy ndarray\n",
    "          : b: the bias - numpy ndarray\n",
    "          : X: the batch - numpy ndarray\n",
    "    Outputs: z: outputs - numpy ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    z = ...\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the output \n",
    "\n",
    "To obtain classification probabilities, we use the softmax function:\n",
    "$$ \\mathbf{o} = softmax(\\mathbf{z}) \\text{         with          } o_i = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)} $$\n",
    "\n",
    "The usual difficulty with the softmax function is the possibility of overflow when the scores $z_i$ are already large. Since a softmax is not affected by a shift affecting the whole vector $\\mathbf{z}$:\n",
    "$$ \\frac{\\exp(z_i - c)}{\\sum_{j=0}^{9} \\exp(z_j - c)} =  \\frac{\\exp(c) \\exp(z_i)}{\\exp(c) \\sum_{j=0}^{9} \\exp(z_j)} = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)}$$\n",
    "what trick can we use to ensure we will not encounter any overflow ? \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Perform the softmax transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - numpy ndarray\n",
    "    Outputs: out: the activation values - numpy ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    out = ...\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making updates\n",
    "\n",
    "We define a learning rate $\\eta$. The goal is to be able to apply updates:\n",
    "$$ \\mathbf{W}^{t+1} = \\mathbf{W}^{t} + \\nabla_{\\mathbf{W}} l_{MLE} $$\n",
    "\n",
    "In order to do this, we will compute this gradient (and the bias) in the function ```update```. In the next function ```updateParams```, we will actually apply the update with regularization. \n",
    "\n",
    "Reminder: the gradient $\\nabla_{\\mathbf{W}} l_{MLE}$ is the matrix containing the partial derivatives \n",
    "$$ \\left[\\frac{\\delta l_{MLE}}{\\delta W_{ij}}\\right]_{i=1..784, j=1..10} $$\n",
    "**Remark**: Careful, the usual way of implementing this in python has the dimensions of $\\mathbf{W}$ reversed compared to the notation of the slides.\n",
    "\n",
    "Coordinate by coordinate, we obtain the following update: \n",
    "$$ W_{ij}^{t+1} = W_{ij}^{t} + \\eta \\frac{\\delta l_{MLE}}{\\delta W_{ij}} $$\n",
    "\n",
    "Via the chain rule, we obtain, for an input feature $i \\in [0, 783]$ and a output class $j \\in [0, 9]$: $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = \\frac{\\delta l_{MLE}}{\\delta z_{j}} \\frac{\\delta z_j}{\\delta W_{ij}}$$ \n",
    "\n",
    "It's easy to compute that $\\frac{\\delta z_j}{\\delta W_{ij}} = x_i$\n",
    "\n",
    "We compute the softmax derivative, to obtain:\n",
    "$$ \\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y} $$\n",
    "\n",
    "Hence, $\\frac{\\delta l_{MLE}}{\\delta z_{j}} = o_j - y_j$ and we obtain that $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = (o_j - y_j) x_i$$\n",
    "\n",
    "This can easily be written as a scalar product, and a similar computation (even easier, actually) can be done for $\\mathbf{b}$. Noting $\\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y}$ as ```grad``` in the following function, compute the gradients $\\nabla_{\\mathbf{W}} l_{MLE}$ and $\\nabla_{\\mathbf{b}} l_{MLE}$ in order to call the function ```updateParams```.\n",
    "\n",
    "Note: the regularizer and the weight_decay $\\lambda$ are used in ```updateParams```.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, W, b, grad, X, regularizer, weight_decay):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : W: the weights - ndarray\n",
    "          : b: the bias -  ndarray\n",
    "          : grad: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : X: the data -  ndarray\n",
    "          : regularizer: 'L2' or None - the regularizer to be used in updateParams\n",
    "          : weight_decay: the weight decay to be used in updateParams - float\n",
    "    Outputs: W: the weights updated -  ndarray\n",
    "           : b: the bias updated -  ndarray\n",
    "    \"\"\"\n",
    "    grad_w = ...\n",
    "    grad_b = ...\n",
    "        \n",
    "    W = updateParams(W, grad_w, eta, regularizer, weight_decay)\n",
    "    b = updateParams(b, grad_b, eta, regularizer, weight_decay)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule is affected by regularization. We implement two cases: No regularization, or L2 regularization. Use the two possible update rules to implement the following function: <div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(param, grad_param, eta, regularizer=None, weight_decay=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: param: the network parameters - ndarray\n",
    "          : grad_param: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : weight_decay: the weight-decay - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    if regularizer==None:\n",
    "        grad = ...\n",
    "        return grad\n",
    "    elif regularizer=='L2':\n",
    "        grad = ...\n",
    "        return grad\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Accuracy\n",
    "\n",
    "Here, we simply use the model to predict the class (by taking the argmax of the output !) for every example in ```X```, and count the number of times the model is right, to output the accuracy.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAcc(W, b, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    # Forward propagation\n",
    "    z = ...\n",
    " \n",
    "    # Compute the softmax and the prediction\n",
    "    out = ...\n",
    "    pred = ...\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = ...\n",
    "      \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training\n",
    "\n",
    "The following hyperparameters are given. Next, we can assemble all the function previously defined to implement a training loop. We will train the classifier on **one epoch**, meaning that the model will see each training example once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "eta = 0.01\n",
    "regularizer = 'L2'\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Training\n",
    "log_interval = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures for plotting\n",
    "g_train_acc=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#######################\n",
    "### Learning process ##\n",
    "#######################\n",
    "for j in range(n_training):\n",
    "    # Getting the example\n",
    "    X, y = ...\n",
    "\n",
    "    # Forward propagation\n",
    "    z = ...\n",
    "\n",
    "    # Compute the softmax\n",
    "    out = ...\n",
    "        \n",
    "    # Compute the gradient at the top layer\n",
    "    derror = out - y # This is o - y \n",
    "\n",
    "    # Update the parameters\n",
    "    W, b = ...\n",
    "\n",
    "    if j % log_interval == 0:\n",
    "        # Every log_interval examples, look at the training accuracy\n",
    "        train_accuracy = computeAcc(W, b, train_images, train_labels) \n",
    "\n",
    "        # And the testing accuracy\n",
    "        test_accuracy = computeAcc(W, b, test_images, test_labels) \n",
    "\n",
    "        g_train_acc.append(train_accuracy)\n",
    "        g_valid_acc.append(test_accuracy)\n",
    "        result_line = str(int(j)) + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "        print(result_line)\n",
    "\n",
    "g_train_acc.append(train_accuracy)\n",
    "g_valid_acc.append(valid_accuracy)\n",
    "result_line = \"Final result:\" + \" \" + str(train_accuracy) + \" \" + str(valid_accuracy) + \" \" + str(eta)\n",
    "print(result_line)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the performance of this simple linear classifier ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part: Autoencoder with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder and PCA\n",
    "\n",
    "First, we will try to connect the representation produced by Principal Component Analysis with what is learnt by a simple, linear, autoencoder. We will use the ```scikit-learn``` implementation of the ```PCA``` to obtain the two first components (hint: use the attribute ```.components_```), and visualize them:\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's find the first 2 PCA components\n",
    "num_components = 2\n",
    "pca = PCA(...).fit(...)\n",
    "\n",
    "# Reshape so they resemble images and we can print them\n",
    "eigen_mnist = pca.components_.reshape(...)\n",
    "\n",
    "# Show the reshaped principal components\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(eigen_mnist[0], cmap='gray')\n",
    "ax[0].set_xlabel('First Principal Component')\n",
    "ax[1].imshow(eigen_mnist[1], cmap='gray')\n",
    "ax[1].set_xlabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variance explained by those components\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the visualization in relation to the variance explained by only keeping the two principal components:\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Autoencoder with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use Keras to implement the autoencoder. You can take a look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf) for some basic commands to use keras.\n",
    "\n",
    "In this first case, we implement a **simple linear autoencoder**. Build it in order to have the same capacity as the PCA decomposition (2 hidden dimensions !) we made just above. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_layer = ...\n",
    "\n",
    "# Encoding layer\n",
    "latent_view = ...\n",
    "\n",
    "# Decoding layer\n",
    "output_layer = ...\n",
    "\n",
    "ae_model = Model(input_layer, output_layer, name='ae_model')\n",
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What loss shoud we use ? Choose the usual one and import it directly from Keras. You can use a simple ```SGD``` optimizer, and then compile the model; finally, train it to rebuild images from the original examples. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import ...\n",
    "loss = ...\n",
    "\n",
    "optimizer = SGD(lr=1e-1) \n",
    "ae_model.compile(optimizer=optimizer, loss=loss) \n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "# No noise here - we want to train a simple auto-encoder and compare visually with PCA\n",
    "history = ae_model.fit(...,\n",
    "                       ...,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=1,\n",
    "                       shuffle=True,\n",
    "                       validation_data=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the name of your layer (obtained through the command ```model.summary()```) is ```'layer'```, here is the way to obtained the weights. Visualize the weights of the encoder and compare them to the two components obtained through the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = ae_model.get_layer('layer').get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the two dimensions of the encoder, in a similar manner to the principal components\n",
    "# (after reshaping them as images !)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualize the images rebuilt by the network !\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few images at random: look from n\n",
    "n = np.random.randint(0,len(test_images)-5)\n",
    "\n",
    "# Plot a few images from n  \n",
    "f, ax = plt.subplots(1,5)\n",
    "for i,a in enumerate(range(n,n+5)):\n",
    "    ...\n",
    "    \n",
    "# Get the prediction from the model \n",
    "\n",
    "\n",
    "# ... and plot them \n",
    "f, ax = plt.subplots(1,5)\n",
    "for i,a in enumerate(range(n,n+5)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same ( = build a new model) with a latent dimension that is largely higher than 2. Compare the visualizations and the images that are rebuilt. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: De-noising Autoencoder\n",
    "\n",
    "Now, we can implement a **de-noising autoencoder**. The following function will transform an array of images by adding it random noise. Create a new autoencoder model, this time with **more layers** and **non-linear activations** (like the ReLU) and train it to rebuild the de-noised images. Display some testing images, with noise, and re-built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(array):\n",
    "    \"\"\"\n",
    "    Adds random noise to each image in the supplied array.\n",
    "    \"\"\"\n",
    "    noise_factor = 0.4\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "    return noisy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data with added noise\n",
    "noisy_train_images = noise(train_images)\n",
    "noisy_test_images = noise(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the images with noise against the originals\n",
    "\n",
    "\n",
    "# Build a new model with more layers and Relu activations\n",
    "\n",
    "\n",
    "# Compile it but here, use noised data as inputs !\n",
    "\n",
    "\n",
    "# Visualize the images rebuilt by the model !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we normalize the images to be in the 0-1 range, what other loss function could we use ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP4_1_empty.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
