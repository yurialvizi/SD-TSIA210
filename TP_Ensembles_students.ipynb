{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Decision trees and ensemble methods\n",
    "\n",
    "In the first part, you are asked to implement a **Stump**, i.e. a decision tree of depth 1, from scratch. Our tree should be able to handle weighted and unweighted samples, in order for it to be used as a weak learner for\n",
    "**AdaBoost** in the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T14:38:25.389Z",
     "iopub.status.busy": "2022-05-27T14:38:25.381Z",
     "iopub.status.idle": "2022-05-27T14:38:25.605Z",
     "shell.execute_reply": "2022-05-27T14:38:25.617Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates 3 synthetic datasets, in which there are just two features, for binary classification. We ask you to provide the solutions to the exercises on all elements of the list ```datasets```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T14:38:25.783Z",
     "iopub.status.busy": "2022-05-27T14:38:25.776Z",
     "iopub.status.idle": "2022-05-27T14:38:26.117Z",
     "shell.execute_reply": "2022-05-27T14:38:26.155Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plot_colors = \"rb\"\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [linearly_separable,\n",
    "            make_moons(n_samples = 200, noise=0.2, random_state=0),\n",
    "            make_circles(n_samples = 200, noise=0.3, factor=0.5, random_state=1)\n",
    "            ]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20,5))\n",
    "for fi in range(len(datasets)):\n",
    "    dataset = datasets[fi]\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    n_classes = len(np.unique(y))\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        ax[fi].scatter(X[idx, 0], X[idx, 1], c=color,  cmap=plt.cm.RdYlBu, edgecolor='black', s=100)\n",
    "dataset = datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T15:56:26.684Z",
     "iopub.status.busy": "2022-05-27T15:56:26.676Z",
     "iopub.status.idle": "2022-05-27T15:56:26.695Z",
     "shell.execute_reply": "2022-05-27T15:56:26.703Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def plot_tree(clf, X, y):\n",
    "    n_classes = 2\n",
    "    plot_colors = \"rb\"\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.axis(\"tight\")\n",
    "    # Plot the training points\n",
    "    for i, color in zip([-1,1], plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color,  cmap=plt.cm.Paired)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## First part: Stumps\n",
    "\n",
    "In this first part, you will code your own class to handle stumps for 2-dimensional data and binary classification (i.e., the class takes value in $\\{0, 1\\}$). You can find the outline of the code you will need to write, following these instructions:\n",
    "- Implement the ```fit``` method : since the weak learners are potentially executed a large number of times, efficiency is crucial. Use the incremental evaluation of the partitions seen in class : the complexity should be  $O(ndc)$ instead of the naive $O(n^2 dc)$ version. We assume that the sorting operations such as ```argsort``` are free because we only need to run them once. Iterate in the 2-dimensions for every possible split, evaluate the quality of each split using an incremental version of the Gini index (defined next) and store the best split. (Note : a non-incremental version will be graded with half the points)\n",
    "- Implement the ```gini``` method : Implement the Gini index coefficient for the case in which there are only 2 classes. In class, you saw the unweighted case: we quickly recall the unweighted then weighted version. Let $C$ be the number of different classes, $p_k(S)$ be the ratio of datapoints of class $k$ in region $S$. Then, the Gini index $G(S)$ is $$ G(S) = 1 - \\sum_{k=1}^{C} p_k(S)^2$$ Given a split in which we have left and right regions $S_r$, $S_l$, let $N_r$ (resp. $N_l$) be the number of datapoints in $S_r$ (resp. $S_l$). The Gini index of the split is the combination of the Gini index of both regions: $$ \\frac{N_r}{N_r + N_l}G(S_r) + \\frac{N_l}{N_r + N_l}G(S_l) $$ For generalizing to the weighted case, let $w_k(S)$ be the sum of the weights of all data-points of class $k$ in $S$. The Gini index is defined as follows: $$ G(S) = 1 - \\sum_{k=1}^{C} \\left( \\frac{w_k(S)}{  \\sum_{m=1}^{C} w_m(S)} \\right)^2 $$ Given a split in which we have left and right regions $S_r$, $S_l$, let $W_r = \\sum_{k=1}^{C} w_k(S)$ (resp. $W_l$) the total weight on $S_r$ (resp. $S_l$). The Gini index of the partition is the combination of the Gini index of both regions: $$ \\frac{W_r}{W_r + W_l}G(S_r) + \\frac{W_l}{W_r + W_l}G(S_l) $$\n",
    "- Implement the ```predict``` method. The input is an array of $n$ $d$-dimensional observations. The output is a ```np.array``` of length $n$. Once the ```predict``` method is coded, use the given function ```plot_tree(my_stump, X, y)``` to plot ```my_stump``` on each of the ```datasets```.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T16:08:27.328Z",
     "iopub.status.busy": "2022-05-27T16:08:27.320Z",
     "iopub.status.idle": "2022-05-27T16:08:27.727Z",
     "shell.execute_reply": "2022-05-27T16:08:27.736Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "class My_stump:\n",
    "    def __init__(self):\n",
    "        self.best_dimension = None\n",
    "        self.best_threshold = None\n",
    "        self.class_below_threshold = None\n",
    "        self.class_above_threshold = None\n",
    "        self.best_ev = None\n",
    "        \n",
    "    def get_threshold(self): return self.best_threshold\n",
    "  \n",
    "    def get_best(self): return self.best_dimension,  self.best_threshold, self.best_ev\n",
    "    \n",
    "    def gini_impurity(): return 0\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "    \n",
    "    def predict(self, X) :\n",
    "        res = []\n",
    "        return np.array(res)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using ```DecisionTreeClassifier``` in ```sklearn```, fit a stump on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[1]\n",
    "X = dataset[0]\n",
    "y = dataset[1]*2-1 \n",
    "w = np.ones(len(X)) \n",
    "\n",
    "my_stump = My_stump()\n",
    "my_stump.fit(X, y, w)\n",
    "my_stump.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the correctness of your code: With ```tree.plot_tree(sk_stump)``` compare the dimension of the split, the threshold and the Gini indices obtained through your own code and those of ```sk_stump```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your code\n",
    "sk_stump = tree.DecisionTreeClassifier(max_depth=1).fit(X, y, sample_weight=w)\n",
    "my_stump = My_stump()\n",
    "my_stump.fit(X, y, w)\n",
    "\n",
    "print(\"Difference in the best threshold:\", sk_stump.tree_.threshold[0] -  my_stump.get_threshold())\n",
    "\n",
    "plot_tree(my_stump,X,y)\n",
    "plot_tree(sk_stump,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Second part: Adaboost\n",
    "\n",
    "Implement the **AdaBoost** seen in class. You can use the ```my_stump``` implemented before. We encourage you to check the correctness by comparing it to ```sk_stump```.\n",
    "- Implement the ```fit``` function in the provided template using the utility functions.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute the error rate of a weak classifier with weights w_m\n",
    "def compute_error(y, y_pred, w_m):\n",
    "    return # TODO - your code here\n",
    "\n",
    "# Compute alpha \n",
    "def compute_alpha(error):\n",
    "    return # TODO - your code here\n",
    "\n",
    "# Compute the update of the weights\n",
    "def update_weights(w_i, alpha, y, y_pred, error):\n",
    "    return # TODO - your code here\n",
    "\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self):\n",
    "        self.alphas = []\n",
    "        self.G_M = []\n",
    "        self.M = None\n",
    "        self.loss_ensemble = []\n",
    "        \n",
    "    def predict(self, X):\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.M)) \n",
    "        for m in range(len(self.G_M)):\n",
    "            y_pred_m = self.G_M[m].predict(X) * self.alphas[m]\n",
    "            weak_preds.iloc[:,m] = y_pred_m\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "        return y_pred.values\n",
    "\n",
    "    def fit(self, X, y, M = 10):\n",
    "        self.alphas = [] \n",
    "        self.training_errors = []\n",
    "        self.M = M\n",
    "        Z=1\n",
    "        for m in range(0, M):# Iterate over M weak classifiers\n",
    "            if m == 0:\n",
    "                w_m = np.ones(len(y)) * 1 / len(y)  \n",
    "            else:\n",
    "                w_m = update_weights(w_m, alpha_m, y, y_pred, error_m)\n",
    "            #\n",
    "            #\n",
    "            # TODO - your code here\n",
    "            #\n",
    "            #\n",
    "            self.G_M.append(G_m) # Save to list of weak classifiers\n",
    "            self.alphas.append(alpha_m)\n",
    "            Z = Z * (2 * np.sqrt(error_m * (1 - error_m)))\n",
    "            self.loss_ensemble.append(Z) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the code for 20 iterations. At each iteration, plot the result of the ensemble of AdaBoost ```ab``` using ```plot_tree(ab, X, y)```\n",
    "- Plot the evolution of the loss in the 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ab = AdaBoost()\n",
    "ab.fit(X, y, M = 20)\n",
    "plt.plot(ab.loss_ensemble)\n",
    "plt.show()\n",
    "plot_tree(ab, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
